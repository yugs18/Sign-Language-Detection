{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43b5426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, BatchNormalization, LSTM, Bidirectional, Dropout, Dense, Input, GlobalAveragePooling1D, Multiply, Permute, RepeatVector, Flatten, Activation, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.regularizers import l2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebf6a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "data = np.load(\"../dataset.npz\")\n",
    "X_train, y_train = data['X_train'], data['y_train']\n",
    "X_val, y_val = data['X_val'], data['y_val']\n",
    "X_test, y_test = data['X_test'], data['y_test']\n",
    "\n",
    "# Debugging: Print shapes and unique classes\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb77915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "input_shape = X_train.shape[1:]  # (sequence_length, features)\n",
    "num_classes = y_train.shape[1]     # Number of output classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16e8b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape data to include the channel dimension (features)\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_val   = X_val.reshape(X_val.shape[0], X_val.shape[1], 1)\n",
    "X_test  = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# Debugging: Verify reshaped data\n",
    "print(f\"Reshaped X_train shape: {X_train.shape}\")\n",
    "print(f\"Reshaped X_val shape: {X_val.shape}\")\n",
    "print(f\"Reshaped X_test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc333f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_over_time(x):\n",
    "    return tf.keras.backend.sum(x, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef62c70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple self-attention block\n",
    "def attention_3d_block(inputs):\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    # Learn an attention vector of size (time_steps, 1)\n",
    "    a = Dense(1, activation='tanh')(inputs)\n",
    "    a = Flatten()(a)\n",
    "    a = Activation('softmax')(a)\n",
    "    a = RepeatVector(input_dim)(a)\n",
    "    a = Permute([2, 1])(a)\n",
    "    # Apply the attention weights\n",
    "    output = Multiply()([inputs, a])\n",
    "    # Sum over time steps to get a context vector\n",
    "    output = Lambda(sum_over_time)(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff59ac38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input layer\n",
    "inputs = Input(shape=(X_train.shape[1], 1))\n",
    "\n",
    "# --- Model Architecture --- #\n",
    "# First Bidirectional LSTM block\n",
    "x = Bidirectional(LSTM(256, return_sequences=True, kernel_regularizer=l2(0.0005)))(inputs)\n",
    "x = Dropout(0.4)(x)\n",
    "\n",
    "# First CNN block\n",
    "x = Conv1D(256, kernel_size=3, activation='relu', kernel_regularizer=l2(0.0005))(x)\n",
    "x = MaxPooling1D(pool_size=2)(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "# Second Bidirectional LSTM block\n",
    "x = Bidirectional(LSTM(128, return_sequences=True, kernel_regularizer=l2(0.0005)))(x)\n",
    "x = Dropout(0.4)(x)\n",
    "\n",
    "# Second CNN block\n",
    "x = Conv1D(128, kernel_size=3, activation='relu', kernel_regularizer=l2(0.0005))(x)\n",
    "x = MaxPooling1D(pool_size=2)(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "# Third Bidirectional LSTM block\n",
    "x = Bidirectional(LSTM(64, return_sequences=True, kernel_regularizer=l2(0.0005)))(x)\n",
    "x = Dropout(0.4)(x)\n",
    "\n",
    "# Third CNN block\n",
    "x = Conv1D(64, kernel_size=3, activation='relu', kernel_regularizer=l2(0.0005))(x)\n",
    "x = MaxPooling1D(pool_size=2)(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "# Final LSTM block\n",
    "x = LSTM(32, return_sequences=True, kernel_regularizer=l2(0.0005))(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "# --- Attention Block --- #\n",
    "attention_output = attention_3d_block(x)\n",
    "\n",
    "# Dense layers after attention\n",
    "x = Dense(128, activation='relu', kernel_regularizer=l2(0.0005))(attention_output)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "# Output layer\n",
    "outputs = Dense(num_classes, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7d85f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and compile the model\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=Adam(learning_rate=0.0002), \n",
    "            loss='categorical_crossentropy', \n",
    "            metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Debugging: Verify model output shape\n",
    "print(f\"Model output shape: {model.output_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745fd033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "checkpoint = ModelCheckpoint(\"../model/best_model.keras\", monitor=\"val_accuracy\", save_best_only=True, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dc21ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "history = model.fit(\n",
    "    X_train, y_train, \n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=128, \n",
    "    callbacks=[early_stopping, reduce_lr, checkpoint]\n",
    ")\n",
    "\n",
    "# Debugging: Print final training and validation accuracy\n",
    "print(f\"Final Training Accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31c1bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n",
    "\n",
    "# --- Confusion Matrix and Classification Report --- #\n",
    "# Convert predictions and true labels to class indices\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=range(num_classes), yticklabels=range(num_classes))\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Print classification report\n",
    "class_report = classification_report(y_true_classes, y_pred_classes)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2213c77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "history_dict = history.history\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_dict[\"accuracy\"], label=\"Training Accuracy\", color=\"blue\")\n",
    "plt.plot(history_dict[\"val_accuracy\"], label=\"Validation Accuracy\", color=\"red\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training & Validation Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_dict[\"loss\"], label=\"Training Loss\", color=\"blue\")\n",
    "plt.plot(history_dict[\"val_loss\"], label=\"Validation Loss\", color=\"red\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cc7fcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534db986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "model.save(\"../model/lstm_cnn_model.keras\")\n",
    "print(\"Model training completed and saved in .keras format.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
